{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to approximate a classical kernel with a quantum computer {#classical_kernels}\n=============================================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Finding a QK to approximate the Gaussian\nkernel. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/toy_qek.png>\n:::\n\n::: {.related}\ntutorial\\_kernels\\_module Training and evaluating quantum kernels\ntutorial\\_kernel\\_based\\_training Kernel-based training of quantum\nmodels with scikit-learn tutorial\\_expressivity\\_fourier\\_series Quantum\nmodels as Fourier series\n:::\n\n*Author: Elies Gil-Fuster (Xanadu resident). Posted: 01 Mar 2022.*\n\nForget about advantages, supremacies, or speed-ups. Let us understand\nbetter what we can and cannot do with a quantum computer. More\nspecifically, in this demo, we want to look into quantum kernels and ask\nwhether we can replicate classical kernel functions with a quantum\ncomputer. Lots of researchers have lengthily stared at the opposite\nquestion, namely that of classical simulation of quantum algorithms.\nYet, by studying what classes of functions we can realize with quantum\nkernels, we can gain some insight into their inner workings.\n\nUsually, in quantum machine learning (QML), we use parametrized quantum\ncircuits (PQCs) to find good functions, whatever *good* means here.\nSince kernels are just one specific kind of well-defined functions, the\ntask of finding a quantum kernel (QK) that approximates a given\nclassical one could be posed as an optimization problem. One way to\nattack this task is to define a loss function quantifying the distance\nbetween both functions (the classical kernel function and the PQC-based\nhypothesis). This sort of approach does not help us much to gain\ntheoretical insights about the structure of kernel-emulating quantum\ncircuits, though.\n\nIn order to build intuition, we will instead study the link between\nclassical and quantum kernels through the lens of the Fourier\nrepresentation of a kernel, which is a common tool in classical machine\nlearning. Two functions can only have the same Fourier spectrum if they\nare the same function. It turns out that, for certain classes of quantum\ncircuits, [we can theoretically describe the Fourier spectrum rather\nwell](https://pennylane.ai/qml/demos/tutorial_expressivity_fourier_series.html).\n\nUsing this theory, together with some good old-fashioned convex\noptimization, we will derive a quantum circuit that approximates the\nfamous Gaussian kernel.\n\nIn order to keep the demo short and sweet, we focus on one simple\nexample. The same ideas apply to more general scenarios. Also, Refs.,,\nand should be helpful for those who\\'d like to see the underlying theory\nof QKs (and also so-called *Quantum Embedding Kernels*) and their\nFourier representation. So tag along if you\\'d like to see how we build\na quantum kernel that approximates the well-known Gaussian kernel\nfunction!\n\n| \n\n![Schematic of the steps covered in this\ndemo.](../demonstrations/classical_kernels/flowchart.PNG){.align-center\nwidth=\"60.0%\"}\n\nKernel-based Machine Learning\n-----------------------------\n\nWe will not be reviewing all the notions of kernels in-depth here.\nInstead, we only need to know that an entire branch of machine learning\nrevolves around some functions we call kernels. If you\\'d like to learn\nmore about where these functions come from, why they\\'re important, and\nhow we can use them (e.g. with PennyLane), check out the following\ndemos, which cover different aspects extensively:\n\n1.  [Training and evaluating quantum\n    kernels](https://pennylane.ai/qml/demos/tutorial_kernels_module.html)\n2.  [Kernel-based training of quantum models with\n    scikit-learn](https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html)\n\nFor the purposes of this demo, a *kernel* is a real-valued function of\ntwo variables $k(x_1,x_2)$ from a given data domain $x_1,\nx_2\\in\\mathcal{X}$. In this demo, we\\'ll deal with real vector spaces as\nthe data domain $\\mathcal{X}\\subseteq\\mathbb{R}^d$, of some dimension\n$d$. A kernel has to be symmetric with respect to exchanging both\nvariables $k(x_1,x_2) = k(x_2,x_1)$. We also enforce kernels to be\npositive semi-definite, but let\\'s avoid getting lost in mathematical\nlingo. You can trust that all kernels featured in this demo are positive\nsemi-definite.\n\nShift-invariant kernels\n-----------------------\n\nSome kernels fulfill another important restriction, called\n*shift-invariance*. Shift-invariant kernels are those whose value\ndoesn\\'t change if we add a shift to both inputs. Explicitly, for any\nsuitable shift vector $\\zeta\\in\\mathcal{X}$, shift-invariant kernels are\nthose for which $k(x_1+\\zeta,x_2+\\zeta)=k(x_1,x_2)$ holds. Having this\nproperty means the function can be written in terms of only one\nvariable, which we call the *lag vector*\n$\\delta:=x_1-x_2\\in\\mathcal{X}$. Abusing notation a bit:\n\n$$k(x_1,x_2)=k(x_1-x_2,0) = k(\\delta).$$\n\nFor shift-invariant kernels, the exchange symmetry property\n$k(x_1,x_2)=k(x_2,x_1)$ translates into reflection symmetry\n$k(\\delta)=k(-\\delta)$. Accordingly, we say $k$ is an *even function*.\n\nWarm up: Implementing the Gaussian kernel\n-----------------------------------------\n\nFirst, let\\'s introduce a simple classical kernel that we will\napproximate on the quantum computer. Start importing the usual suspects:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nimport matplotlib.pyplot as plt\nimport math\nnp.random.seed(53173)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\\'ll look at the Gaussian kernel:\n$k_\\sigma(x_1,x_2):=e^{-\\lVert x_1-x_2\\rVert^2/2\\sigma^2}$. This\nfunction is clearly shift-invariant:\n\n$$\\begin{aligned}\nk_\\sigma(x_1+\\zeta,x_2+\\zeta) &= e^{-\\lVert(x_1+\\zeta)-(x_2+\\zeta)\\rVert^2/2\\sigma^2} \\\\\n& = e^{-\\lVert x_1-x_2\\rVert^2/2\\sigma^2} \\\\\n& = k_\\sigma(x_1,x_2).\n\\end{aligned}$$\n\nThe object of our study will be a simple version of the Gaussian kernel,\nwhere we consider $1$-dimensional data, so $\\lVert\nx_1-x_2\\rVert^2=(x_1-x_2)^2$. Also, we take $\\sigma=1/\\sqrt{2}$ so that\nwe further simplify the exponent. We can always re-introduce it later by\nrescaling the data. Again, we can write the function in terms of the lag\nvector only:\n\n$$k(\\delta)=e^{-\\delta^2}.$$\n\nNow let\\'s write a few lines to plot the Gaussian kernel:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gaussian_kernel(delta):\n    return math.exp(-delta ** 2)\n\ndef make_data(n_samples, lower=-np.pi, higher=np.pi):\n    x = np.linspace(lower, higher, n_samples)\n    y = np.array([gaussian_kernel(x_) for x_ in x])\n    return x,y\n\nX, Y_gaussian = make_data(100)\n\nplt.plot(X, Y_gaussian)\nplt.suptitle(\"The Gaussian kernel with $\\sigma=1/\\sqrt{2}$\")\nplt.xlabel(\"$\\delta$\")\nplt.ylabel(\"$k(\\delta)$\")\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this demo, we will consider only this one example. However, the\narguments we make and the code we use are also amenable to any kernel\nwith the following mild restrictions:\n\n1.  Shift-invariance\n2.  Normalization $k(0)=1$.\n3.  Smoothness (in the sense of a quickly decaying Fourier spectrum).\n\nNote that is a very large class of kernels! And also an important one\nfor practical applications.\n\nFourier analysis of the Gaussian kernel\n=======================================\n\nThe next step will be to find the Fourier spectrum of the Gaussian\nkernel, which is an easy problem for classical computers. Once we\\'ve\nfound it, we\\'ll build a QK that produces a finite Fourier series\napproximation to that spectrum.\n\nLet\\'s briefly recall that a Fourier series is the representation of a\nperiodic function using the sine and cosine functions. Fourier analysis\ntells us that we can write any given periodic function as\n\n$$f(x) = a_0 + \\sum_{n=1}^\\infty a_n\\cos(n\\omega_0x) + b_n\\sin(n\\omega_0x).$$\n\nFor that, we only need to find the suitable base frequency $\\omega_0$\nand coefficients $a_0, a_1, \\ldots, b_0, b_1,\\ldots$.\n\nBut the Gaussian kernel is an aperiodic function, whereas the Fourier\nseries only makes sense for periodic functions!\n\n*What can we do?!*\n\nWe can cook up a periodic extension to the Gaussian kernel, for a given\nperiod $2L$ (we take $L=\\pi$ as default):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def Gauss_p(x, L=np.pi):\n    # Send x to x_mod in the period around 0\n    x_mod = np.mod(x+L, 2*L) - L\n    return gaussian_kernel(x_mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "which we can now plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_func = np.linspace(-10, 10, 321)\ny_func = [Gauss_p(x) for x in x_func]\n\nplt.plot(x_func, y_func)\nplt.xlabel(\"$\\delta$\")\nplt.suptitle(\"Periodic extension to the Gaussian kernel\")\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practice, we would construct several periodic extensions of the\naperiodic function, with increasing periods. This way, we can study the\nbehaviour when the period approaches infinity, i.e. the regime where the\nfunction stops being periodic.\n\nNext up, how does the Fourier spectrum of such an object look like? We\ncan find out using PennyLane\\'s `fourier` module!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pennylane.fourier import coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function `coefficients` computes for us the coefficients of the\nFourier series up to a fixed term. One tiny detail here: `coefficients`\nreturns one complex number $c_n$ for each frequency $n$. The real part\ncorresponds to the $a_n$ coefficient, and the imaginary part to the\n$b_n$ coefficient: $c_n=a_n+ib_n$. Because the Gaussian kernel is an\neven function, we know that the imaginary part of every coefficient will\nbe zero, so $c_n=a_n$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fourier_p(d):\n    \"\"\"\n    We only take the first d coefficients [:d]\n    because coefficients() treats the negative frequencies\n    as different from the positive ones.\n    For real functions, they are the same.\n    \"\"\"\n    return np.real(coefficients(Gauss_p, 1, d-1)[:d])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are restricted to considering only a finite number of Fourier terms.\nBut isn\\'t that problematic, one may say? Well, maybe. Since we know the\nGaussian kernel is a smooth function, we expect that the coefficients\nconverge to $0$ at some point, and we will only need to consider terms\nup to this point. Let\\'s look at the coefficients we obtain by setting a\nlow value for the number of coefficients and then slowly letting it\ngrow:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = [0]\nfor n in range(2,7):\n    N.append(n)\n    F = fourier_p(n)\n    plt.plot(N, F, 'x', label='{}'.format(n))\n\nplt.legend()\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient $c_n$\")\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What do we see? For very small coefficient counts, like $2$ and $3$, we\nsee that the last allowed coefficient is still far from $0$. That\\'s a\nvery clear indicator that we need to consider more frequencies. At the\nsame time, it seems like starting at $5$ or $6$ all the non-zero\ncontributions have already been well captured. This is important for us,\nsince it tells us the minimum number of qubits we should use. One can\nsee that every new qubit doubles the number of frequencies we can use,\nso for $n$ qubits, we will have $2^n$. At minimum of $6$ frequencies\nmeans at least $3$ qubits, corresponding to $2^3=8$ frequencies. As\nwe\\'ll see later, we\\'ll work with $5$ qubits, so $32$ frequencies. That\nmeans the spectrum we will be trying to replicate will be the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(range(32), fourier_p(32), 'x')\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient $c_n$\")\nplt.suptitle(\"Fourier spectrum of the Gaussian kernel\")\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just need a QK with the same Fourier spectrum!\n\nDesigning a suitable QK\n=======================\n\nDesigning a suitable QK amounts to designing a suitable parametrized\nquantum circuit. Let\\'s take a moment to refresh the big scheme of\nthings with the following picture:\n\n| \n\n![The quantum kernel considered in this\ndemo.](../demonstrations/classical_kernels/QEK.jpg){.align-center\nwidth=\"70.0%\"}\n\nWe construct the quantum kernel from a quantum embedding (see the demo\non [Quantum Embedding\nKernels](pennylane.ai/qml/demos/tutorial_kernels_module.html)). The\nquantum embedding circuit will consist of two parts. The first one,\ntrainable, will be a parametrized general state preparation scheme\n$W_a$, with parameters $a$. In the second one, we input the data,\ndenoted by $S(x)$.\n\nStart with the non-trainable gate we\\'ll use to encode the data $S(x)$.\nIt consists of applying one Pauli-$Z$ rotation to each qubit with\nrotation parameter $x$ times some constant $\\vartheta_i$, for the\n$i^\\text{th}$ qubit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def S(x, thetas, wires):\n    for (i, wire) in enumerate(wires):\n        qml.RZ(thetas[i] * x, wires = [wire])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By setting the `thetas` properly, we achieve the integer-valued\nspectrum, as required by the Fourier series expansion of a function of\nperiod $2\\pi$: $\\{0, 1, \\ldots, 2^n-2, 2^n-1\\}$, for $n$ qubits. Some\nmath shows that setting $\\vartheta_i=2^{n-i}$, for $\\{1,\\ldots,n\\}$\nproduces the desired outcome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_thetas(n_wires):\n    return [2 ** i for i in range(n_wires-1, -1, -1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we introduce the only trainable gate we need to make use of.\nContrary to the usual Ans\u00e4tze used in supervised and unsupervised\nlearning, we use a state preparation template called\n`MottonenStatePreparation`. This is one option for amplitude encoding\nalready implemented in PennyLane, so we don\\'t need to code it\nourselves. Amplitude encoding is a common way of embedding classical\ndata into a quantum system in QML. The unitary associated to this\ntemplate transforms the $\\lvert0\\rangle$ state into a state with\namplitudes $a=(a_0,a_1,\\ldots,a_{2^n-1})$, namely\n$\\lvert a\\rangle=\\sum_j a_j\\lvert j\\rangle$, provided\n$\\lVert a\\rVert^2=1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def W(features, wires):\n    qml.templates.state_preparations.MottonenStatePreparation(features, wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With that, we have the feature map onto the Hilbert space of the quantum\ncomputer:\n\n$$\\lvert x_a\\rangle = S(x)W_a\\lvert0\\rangle,$$\n\nfor a given $a$, which we will specify later.\n\nAccordingly, we can build the QK corresponding to this feature map as\n\n$$\\begin{aligned}\nk_a(x_1,x_2) &= \\lvert\\langle0\\rvert W_a^\\dagger S^\\dagger(x_1)\nS(x_2)W_a\\lvert0\\rangle\\rvert^2 \\\\\n&= \\lvert\\langle0\\rvert W_a^\\dagger S(x_2-x_1) W_a\\lvert0\\rangle\\rvert^2.\n\\end{aligned}$$\n\nIn the code below, the variable `amplitudes` corresponds to our set $a$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ansatz(x1, x2, thetas, amplitudes, wires):\n    W(amplitudes, wires)\n    S(x1 - x2, thetas, wires)\n    qml.adjoint(W)(amplitudes, wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this kernel is by construction real-valued, we also have\n\n$$\\begin{aligned}\n(k_a(x_1,x_2))^\\ast &= k_a(x_1,x_2) \\\\\n&= \\lvert\\langle0\\rvert W_a^\\dagger S(x_1-x_2) W_a\\lvert0\\rangle\\rvert^2 \\\\\n&= k_a(x_2,x_1).\n\\end{aligned}$$\n\nFurther, this QK is also shift-invariant $k_a(x_1,x_2) = k_a(x_1+\\zeta,\nx_2+\\zeta)$ for any $\\zeta\\in\\mathbb{R}$. So we can also write it in\nterms of the lag $\\delta=x_1-x_2$:\n\n$$k_a(\\delta) = \\lvert\\langle0\\rvert W_a^\\dagger\nS(\\delta)W_a\\lvert0\\rangle\\rvert^2.$$\n\nSo far, we only wrote the gate layout for the quantum circuit, no\nmeasurement! We need a few more functions for that!\n\nComputing the QK function on a quantum device\n=============================================\n\nAlso, at this point, we need to set the number of qubits of our\ncomputer. For this example, we\\'ll use the variable `n_wires`, and set\nit to $5$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_wires = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the quantum simulator:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires = n_wires, shots = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we construct the quantum node:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef QK_circuit(x1, x2, thetas, amplitudes):\n    ansatz(x1, x2, thetas, amplitudes, wires = range(n_wires))\n    return qml.probs(wires = range(n_wires))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall that the output of a QK is defined as the probability of\nobtaining the outcome $\\lvert0\\rangle$ when measuring in the\ncomputational basis. That corresponds to the $0^\\text{th}$ entry of\n`qml.probs`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def QK_2(x1, x2, thetas, amplitudes):\n    return QK_circuit(x1, x2, thetas, amplitudes)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a couple of quality-of-life improvements, we write a function that\nimplements the QK with the lag $\\delta$ as its argument, and one that\nimplements it on a given set of data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def QK(delta, thetas, amplitudes):\n    return QK_2(delta, 0, thetas, amplitudes)\n\ndef QK_on_dataset(deltas, thetas, amplitudes):\n    y = np.array([QK(delta, thetas, amplitudes) for delta in deltas])\n    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is also a good place to fix the `thetas` array, so that we don\\'t\nforget later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetas = make_thetas(n_wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s see how this looks like for one particular choice of\n`amplitudes`. We need to make sure the array fulfills the normalization\nconditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_features = np.asarray([1./(1+i) for i in range(2 ** n_wires)])\ntest_amplitudes = test_features / np.sqrt(np.sum(test_features ** 2))\n\nY_test = QK_on_dataset(X, thetas, test_amplitudes)\n\nplt.plot(X, Y_test)\nplt.xlabel(\"$\\delta$\")\nplt.suptitle(\"QK with test amplitudes\")\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can see that the stationary kernel with this particular initial\nstate has a decaying spectrum that looks similar to $1/\\lvert x\\rvert$\n--- but not yet like a Gaussian.\n\nHow to find the amplitudes emulating a Gaussian kernel\n======================================================\n\nIf we knew exactly which amplitudes to choose in order to build a given\nFourier spectrum, our job would be done here. However, the equations\nderived in the literature are not trivial to solve.\n\nAs mentioned in the introduction, one could just \\\"learn\\\" this\nrelation, that is, tune the parameters of the quantum kernel in a\ngradient-based manner until it matches the classical one.\n\nWe want to take an intermediate route between analytical solution and\nblack-box optimization. For that, we derive an equation that links the\namplitudes to the spectrum we want to construct and then use\nold-fashioned convex optimization to find the solution. If you are not\ninterested in the details, you can just jump to the last plots of this\ndemo and confirm that we can to emulate the Gaussian kernel using the\nansatz for our QK constructed above.\n\nIn order to simplify the formulas, we introduce new variables, which we\ncall `probabilities` $(p_0, p_1, p_2, \\ldots, p_{2^n-1})$, and we define\nas $p_j=\\lvert a_j\\rvert^2$. Following the normalization property above,\nwe have $\\sum_j p_j=1$. Don\\'t get too fond of them, we only need them\nfor this step! Remember we introduced the vector $a$ for the\n`MottonenStatePreparation` as the *amplitudes* of a quantum state? Then\nit makes sense that we call its squares *probabilities*, doesn\\'t it?\n\nThere is a crazy formula that matches the entries of *probabilities*\nwith the Fourier series of the resulting QK function:\n\n$$\\begin{aligned}\n\\text{probabilities} &\\longrightarrow \\text{Fourier coefficients} \\\\\n\\begin{pmatrix} p_0 \\\\ p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_{2^n-1} \\end{pmatrix}\n&\\longmapsto \\begin{pmatrix} \\sum_{j=0}^{2^n-1} p_j^2 \\\\ \\sum_{j=1}^{2^n-1}\np_j p_{j-1} \\\\ \\sum_{j=2}^{2^n-1} p_j p_{j-2} \\\\ \\vdots \\\\ p_{2^n-1} p_0\n\\end{pmatrix}\n\\end{aligned}$$\n\nThis looks a bit scary, it follows from expanding the matrix product\n$W_a^\\dagger S(\\delta)W_a$, and then collecting terms according to\nFourier basis monomials. In this sense, the formula is general and it\napplies to any shift-invariant kernel we might want to approximate, not\nonly the Gaussian kernel.\n\nOur goal is to find the set of $p_j$\\'s that produces the Fourier\ncoefficients of a given kernel function (in our case, the Gaussian\nkernel), namely its spectrum $(s_0, s_1, s_2, \\ldots, s_{2^n-1})$. We\nconsider now a slightly different map $F_s$, for a given spectrum\n$(s_0, s_1, \\ldots, s_{2^n-1})$:\n\n$$\\begin{aligned}\nF_s: \\text{probabilities} &\\longrightarrow \\text{Difference between Fourier\ncoefficients} \\\\\n\\begin{pmatrix} p_0 \\\\ p_1 \\\\ p_2 \\\\ \\vdots \\\\ p_{2^n-1} \\end{pmatrix}\n&\\longmapsto \\begin{pmatrix} \\sum_{j=0}^{2^n-1} p_j^2 - s_0 \\\\\n\\sum_{j=1}^{2^n-1} p_j p_{j-1} - s_1 \\\\ \\sum_{j=2}^{2^n-1} p_j\np_{j-2} - s_2 \\\\ \\vdots \\\\ p_{2^n-1}p_0 - s_{2^n-1} \\end{pmatrix}.\n\\end{aligned}$$\n\nIf you look at it again, you\\'ll see that the zero (or solution) of this\nsecond map $F_s$ is precisely the array of *probabilities* we are\nlooking for. We can write down the first map as:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def predict_spectrum(probabilities):\n    d = len(probabilities)\n    spectrum = []\n    for s in range(d):\n        s_ = 0\n\n        for j in range(s, d):\n            s_ += probabilities[j] * probabilities[j - s]\n\n        spectrum.append(s_)\n\n    # This is to make the output have the same format as\n    # the output of pennylane.fourier.coefficients\n    for s in range(1,d):\n        spectrum.append(spectrum[d - s])\n\n    return spectrum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then $F_s$ is just `predict_spectrum` minus the spectrum we want to\npredict:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def F(probabilities, spectrum):\n    d = len(probabilities)\n    return predict_spectrum(probabilities)[:d] - spectrum[:d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These closed-form equations allow us to find the solution numerically,\nusing Newton\\'s method! Newton\\'s method is a classical one from convex\noptimization theory. For our case, since the formula is quadratic, we\nrest assured that we are within the realm of convex functions.\n\nFinding the solution\n====================\n\nIn order to use Newton\\'s method we need the Jacobian of $F_s$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def J_F(probabilities):\n    d = len(probabilities)\n    J = np.zeros(shape=(d,d))\n    for i in range(d):\n        for j in range(d):\n            if (i + j < d):\n                J[i][j] += probabilities[i + j]\n            if(i - j <= 0):\n                J[i][j] += probabilities[j - i]\n    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Showing that this is indeed $\\nabla F_s$ is left as an exercise for the\nreader. For Newton\\'s method, we also need an initial guess. Finding a\ngood initial guess requires some tinkering; different problems will\nbenefit from different ones. Here is a tame one that works for the\nGaussian kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_initial_probabilities(d):\n    probabilities = np.ones(d)\n    deg = np.array(range(1, d + 1))\n    probabilities = probabilities / deg\n    return probabilities\n\nprobabilities = make_initial_probabilities(2 ** n_wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall the `spectrum` we want to match is that of the periodic extension\nof the Gaussian kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spectrum = fourier_p(2 ** n_wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fix the hyperparameters for Newton\\'s method:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "d = 2 ** n_wires\nmax_steps = 100\ntol = 1.e-20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we\\'re good to go!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for step in range(max_steps):\n    inc = np.linalg.solve(J_F(probabilities), -F(probabilities, spectrum))\n    probabilities = probabilities + inc\n    if (step+1) % 10 == 0:\n        print(\"Error norm at step {0:3}: {1}\".format(step + 1,\n                                               np.linalg.norm(F(probabilities,\n                                                                spectrum))))\n        if np.linalg.norm(F(probabilities, spectrum)) < tol:\n            print(\"Tolerance trespassed! This is the end.\")\n            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tolerance we set was fairly low, one should expect good things to\ncome out of this. Let\\'s have a look at the solution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(range(d), probabilities, 'x')\nplt.xlabel(\"array entry $j$\")\nplt.ylabel(\"probabilities $p_j$\")\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Would you be able to tell whether this is correct? Me neither! But all\nthose probabilities being close to $0$ should make us fear some of them\nmust\\'ve turned negative. That would be fatal for us. For\n`MottonenStatePreparation`, we\\'ll need to give `amplitudes` as one of\nthe arguments, which is the component-wise square root of\n`probabilities`. And hence the problem! Even if they are very small\nvalues, if any entry of `probabilities` is negative, the square root\nwill give `nan`. In order to avoid that, we use a simple thresholding\nwhere we replace very small entries by $0$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def probabilities_threshold_normalize(probabilities, thresh = 1.e-10):\n    d = len(probabilities)\n    p_t = probabilities.copy()\n    for i in range(d):\n        if np.abs(probabilities[i] < thresh):\n            p_t[i] = 0.0\n\n    p_t = p_t / np.sum(p_t)\n\n    return p_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we need to take the square root:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "probabilities = probabilities_threshold_normalize(probabilities)\namplitudes = np.sqrt(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A little plotting never killed nobody\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(range(d), probabilities, '+', label = \"probability $p_j = |a_j|^2$\")\nplt.plot(range(d), amplitudes, 'x', label = \"amplitude $a_j$\")\nplt.xlabel(\"array entry $j$\")\nplt.legend()\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the solution\n========================\n\nAnd the moment of truth! Does the solution really match the spectrum? We\ntry it first using `predict_spectrum` only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(range(d), fourier_p(d)[:d], '+', label = \"Gaussian kernel\")\nplt.plot(range(d), predict_spectrum(probabilities)[:d], 'x', label = \"QK predicted\")\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient\")\nplt.suptitle(\"Fourier spectrum of the Gaussian kernel\")\nplt.legend()\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems like it does! But as we just said, this is still only the\npredicted spectrum. We haven\\'t called the quantum computer at all yet!\n\nLet\\'s see what happens when we call the function `coefficients` on the\nQK function we defined earlier. Good coding practice tells us we should\nprobably turn this step into a function itself, in case it is of use\nlater:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fourier_q(d, thetas, amplitudes):\n    return np.real(coefficients(lambda x: QK(x, thetas, amplitudes), 1, d-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And with this, we can finally visualize how the Fourier spectrum of the\nQK function compares to that of the Gaussian kernel:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(range(d), fourier_p(d)[:d], '+', label = \"Gaussian kernel\")\nplt.plot(range(d), predict_spectrum(probabilities)[:d], 'x', label=\"QK predicted\")\nplt.plot(range(d), fourier_q(d, thetas, amplitudes)[:d], '.', label = \"QK computer\")\nplt.xlabel(\"frequency $n$\")\nplt.ylabel(\"Fourier coefficient\")\nplt.suptitle(\"Fourier spectrum of the Gaussian kernel\")\nplt.legend()\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems it went well! Matching spectra should mean matching kernel\nfunctions, right?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Y_learned = QK_on_dataset(X, thetas, amplitudes)\nY_truth = [Gauss_p(x_) for x_ in X]\n\nplt.plot(X, Y_learned, '-.', label = \"QK\")\nplt.plot(X, Y_truth, '--', label = \"Gaussian kernel\")\nplt.xlabel(\"$\\delta$\")\nplt.ylabel(\"$k(\\delta)$\")\nplt.legend()\nplt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yeah! We did it!\n\n![](../demonstrations/classical_kernels/salesman.PNG){.align-center\nwidth=\"70.0%\"}\n\nReferences\n==========\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}