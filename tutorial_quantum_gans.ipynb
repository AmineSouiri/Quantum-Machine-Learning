{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum GANs {#quantum_gans}\n============\n\n::: {.meta}\n:property=\\\"og:description\\\": Explore quantum GANs to generate\nhand-written digits of zero :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/patch.jpeg>\n:::\n\n::: {.related}\ntutorial\\_QGAN QGANs with Cirq + TensorFlow\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Author: James Ellis. Contact: jamesellis9570\\@yahoo.co.uk. Last\nupdated: 27 Jan 2022.*\n\nIn this tutorial, we will explore quantum GANs to generate hand-written\ndigits of zero. We will first cover the theory of the classical case,\nthen extend to a quantum method recently proposed in the literature. If\nyou have no experience with GANs, particularly in PyTorch, you might\nfind [PyTorch\\'s\ntutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\nuseful since it serves as the foundation for what is to follow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generative Adversarial Networks (GANs)\n======================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of generative adversarial networks (GANs) \\[1\\] is to generate\ndata that resembles the original data used in training. To achieve this,\nwe train two neural networks simulatenously: a generator and a\ndiscriminator. The job of the generator is to create fake data which\nimitates the real training dataset. On the otherhand, the discriminator\nacts like a detective trying to discern real from fake data. During the\ntraining process, both players iteratively improve with one another. By\nthe end, the generator should hopefully generate new data very similar\nto the training dataset.\n\nSpecifically, the training dataset represents samples drawn from some\nunknown data distribution $P_{data}$, and the generator has the job of\ntrying to capture this distribution. The generator, $G$, starts from\nsome initial latent distribution, $P_z$, and maps it to $P_g = G(P_z)$.\nThe best solution would be for $P_g = P_{data}$. However, this point is\nrarely achieved in practice apart from in the most simple tasks.\n\nBoth the discriminator, $D$, and generator, $G$, play in a 2-player\nminimax game. The discriminator tries to maximise the probability of\ndiscerning real from fake data, while the generator tries to minimise\nthe same probability. The value function for the game is summarised by,\n\n$$\\begin{aligned}\n\\begin{align}\n\\min_G \\max_D V(D,G) &= \\mathbb{E}_{\\boldsymbol{x}\\sim p_{data}}[\\log D(\\boldsymbol{x})] \\\\\n    & ~~ + \\mathbb{E}_{\\boldsymbol{z}\\sim p_{\\boldsymbol{z}}}[\\log(1 - D(G(\\boldsymbol{z}))]\n\\end{align}\n\\end{aligned}$$\n\n-   $\\boldsymbol{x}$: real data sample\n-   $\\boldsymbol{z}$: latent vector\n-   $D(\\boldsymbol{x})$: probability of the discriminator classifying\n    real data as real\n-   $G(\\boldsymbol{z})$: fake data\n-   $D(G(\\boldsymbol{z}))$: probability of discriminator classifying\n    fake data as real\n\nIn practice, the two networks are trained iteratively, each with a\nseparate loss function to be minimised,\n\n$$L_D = -[y \\cdot \\log(D(x)) + (1-y)\\cdot \\log(1-D(G(z)))]$$\n\n$$L_G = [(1-y) \\cdot \\log(1-D(G(z)))]$$\n\nwhere $y$ is a binary label for real ($y=1$) or fake ($y=0$) data. In\npractice, generator training is shown to be more stable \\[1\\] when made\nto maximise $\\log(D(G(z)))$ instead of minimising $\\log(1-D(G(z)))$.\nHence, the generator loss function to be minimised becomes,\n\n$$L_G = -[(1-y) \\cdot \\log(D(G(z)))]$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum GANs: The Patch Method\n==============================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we re-create one of the quantum GAN methods presented\nby Huang et al.\\[2\\]: the patch method. This method uses several quantum\ngenerators, with each sub-generator, $G^{(i)}$, responsible for\nconstructing a small patch of the final image. The final image is\ncontructed by concatenting all of the patches together as shown below.\n\n![](../demonstrations/quantum_gans/patch.jpeg){.align-center\nwidth=\"90.0%\"}\n\nThe main advantage of this method is that it is particulary suited to\nsituations where the number of available qubits are limited. The same\nquantum device can be used for each sub-generator in an iterative\nfashion, or execution of the generators can be parallelised across\nmultiple devices.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this tutorial, parenthesised superscripts are used to denote\nindividual objects as part of a collection.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Module Imports\n==============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Library imports\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport pennylane as qml\n\n# Pytorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# Set the random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data\n====\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned in the introduction, we will use a [small\ndataset](https://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits)\nof handwritten zeros. First, we need to create a custom dataloader for\nthis dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DigitsDataset(Dataset):\n    \"\"\"Pytorch dataloader for the Optical Recognition of Handwritten Digits Data Set\"\"\"\n\n    def __init__(self, csv_file, label=0, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.csv_file = csv_file\n        self.transform = transform\n        self.df = self.filter_by_label(label)\n\n    def filter_by_label(self, label):\n        # Use pandas to return a dataframe of only zeros\n        df = pd.read_csv(self.csv_file)\n        df = df.loc[df.iloc[:, -1] == label]\n        return df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        image = self.df.iloc[idx, :-1] / 16\n        image = np.array(image)\n        image = image.astype(np.float32).reshape(8, 8)\n\n        if self.transform:\n            image = self.transform(image)\n\n        # Return image and label\n        return image, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we define some variables and create the dataloader instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "image_size = 8  # Height / width of the square images\nbatch_size = 1\n\ntransform = transforms.Compose([transforms.ToTensor()])\ndataset = DigitsDataset(csv_file=\"quantum_gans/optdigits.tra\", transform=transform)\ndataloader = torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size, shuffle=True, drop_last=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s visualize some of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,2))\n\nfor i in range(8):\n    image = dataset[i][0].reshape(image_size,image_size)\n    plt.subplot(1,8,i+1)\n    plt.axis('off')\n    plt.imshow(image.numpy(), cmap='gray')\n    \nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the Discriminator\n==============================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the discriminator, we use a fully connected neural network with two\nhidden layers. A single output is sufficient to represent the\nprobability of an input being classified as real.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n    \"\"\"Fully connected classical discriminator\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            # Inputs to first hidden layer (num_input_features -> 64)\n            nn.Linear(image_size * image_size, 64),\n            nn.ReLU(),\n            # First hidden layer (64 -> 16)\n            nn.Linear(64, 16),\n            nn.ReLU(),\n            # Second hidden layer (16 -> output)\n            nn.Linear(16, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the Generator\n==========================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sub-generator, $G^{(i)}$, shares the same circuit architecture as\nshown below. The overall quantum generator consists of $N_G$\nsub-generators, each consisting of $N$ qubits. The process from latent\nvector input to image output can be split into four distinct sections:\nstate embedding, parameterisation, non-linear transformation, and\npost-processing. Each of the following sections below refer to a single\niteration of the training process to simplify the discussion.\n\n![](../demonstrations/quantum_gans/qcircuit.jpeg){.align-center\nwidth=\"90.0%\"}\n\n**1) State Embedding**\n\nA latent vector, $\\boldsymbol{z}\\in\\mathbb{R}^N$, is sampled from a\nuniform distribution in the interval $[0,\\pi/2)$. All sub-generators\nreceive the same latent vector which is then embedded using RY gates.\n\n**2) Parameterised Layers**\n\nThe parameterised layer consists of parameterised RY gates followed by\ncontrol Z gates. This layer is repeated $D$ times in total.\n\n**3) Non-Linear Transform**\n\nQuantum gates in the circuit model are unitary which, by definition,\nlinearly transform the quantum state. A linear mapping between the\nlatent and generator distribution would be suffice for only the most\nsimple generative tasks, hence we need non-linear transformations. We\nwill use ancillary qubits to help.\n\nFor a given sub-generator, the pre-measurement quantum state is given\nby,\n\n$$|\\Psi(z)\\rangle = U_{G}(\\theta)|\\boldsymbol{z}\\rangle$$\n\nwhere $U_{G}(\\theta)$ represents the overall unitary of the\nparameterised layers. Let us inspect the state when we take a partial\nmeasurment, $\\Pi$, and trace out the ancillary subsystem, $\\mathcal{A}$,\n\n$$\\rho(\\boldsymbol{z}) = \\frac{\\text{Tr}_{\\mathcal{A}}(\\Pi \\otimes \\mathbb{I} |\\Psi(z)\\rangle \\langle \\Psi(\\boldsymbol{z})|) }{\\text{Tr}(\\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle \\langle \\Psi(\\boldsymbol{z})|))} = \\frac{\\text{Tr}_{\\mathcal{A}}(\\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle \\langle \\Psi(\\boldsymbol{z})|) }{\\langle \\Psi(\\boldsymbol{z})| \\Pi \\otimes \\mathbb{I} |\\Psi(\\boldsymbol{z})\\rangle}$$\n\nThe post-measurement state, $\\rho(\\boldsymbol{z})$, is dependent on\n$\\boldsymbol{z}$ in both the numerator and denominator. This means the\nstate has been non-linearly transformed! For this tutorial,\n$\\Pi = (|0\\rangle \\langle0|)^{\\otimes N_A}$, where $N_A$ is the number\nof ancillary qubits in the system.\n\nWith the remaining data qubits, we measure the probability of\n$\\rho(\\boldsymbol{z})$ in each computational basis state, $P(j)$, to\nobtain the sub-generator output, $\\boldsymbol{g}^{(i)}$,\n\n$$\\boldsymbol{g}^{(i)} = [P(0), P(1), ... ,P(2^{N-N_A} - 1)]$$\n\n**4) Post Processing**\n\nDue to the normalisation constraint of the measurment, all elements in\n$\\boldsymbol{g}^{(i)}$ must sum to one. This is a problem if we are to\nuse $\\boldsymbol{g}^{(i)}$ as the pixel intensity values for our patch.\nFor example, imagine a hypothetical situation where a patch of full\nintensity pixels was the target. The best patch a sub-generator could\nproduce would be a patch of pixels all at a magnitude of\n$\\frac{1}{2^{N-N_A}}$. To alleviate this constraint, we apply a\npost-processing technique to each patch,\n\n$$\\boldsymbol{\\tilde{x}^{(i)}} = \\frac{\\boldsymbol{g}^{(i)}}{\\max_{k}\\boldsymbol{g}_k^{(i)}}$$\n\nTherefore, the final image, $\\boldsymbol{\\tilde{x}}$, is given by\n\n$$\\boldsymbol{\\tilde{x}} = [\\boldsymbol{\\tilde{x}^{(1)}}, ... ,\\boldsymbol{\\tilde{x}^{(N_G)}}]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Quantum variables\nn_qubits = 5  # Total number of qubits / N\nn_a_qubits = 1  # Number of ancillary qubits / N_A\nq_depth = 6  # Depth of the parameterised quantum circuit / D\nn_generators = 4  # Number of subgenerators for the patch method / N_G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the quantum device we want to use, along with any\navailable CUDA GPUs (if available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Quantum simulator\ndev = qml.device(\"lightning.qubit\", wires=n_qubits)\n# Enable CUDA device if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the quantum circuit and measurement process described\nabove.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\ndef quantum_circuit(noise, weights):\n\n    weights = weights.reshape(q_depth, n_qubits)\n\n    # Initialise latent vectors\n    for i in range(n_qubits):\n        qml.RY(noise[i], wires=i)\n\n    # Repeated layer\n    for i in range(q_depth):\n        # Parameterised layer\n        for y in range(n_qubits):\n            qml.RY(weights[i][y], wires=y)\n\n        # Control Z gates\n        for y in range(n_qubits - 1):\n            qml.CZ(wires=[y, y + 1])\n\n    return qml.probs(wires=list(range(n_qubits)))\n\n\n# For further info on how the non-linear transform is implemented in Pennylane\n# https://discuss.pennylane.ai/t/ancillary-subsystem-measurement-then-trace-out/1532\ndef partial_measure(noise, weights):\n    # Non-linear Transform\n    probs = quantum_circuit(noise, weights)\n    probsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\n    probsgiven0 /= torch.sum(probs)\n\n    # Post-Processing\n    probsgiven = probsgiven0 / torch.max(probsgiven0)\n    return probsgiven"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create a quantum generator class to use during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PatchQuantumGenerator(nn.Module):\n    \"\"\"Quantum generator class for the patch method\"\"\"\n\n    def __init__(self, n_generators, q_delta=1):\n        \"\"\"\n        Args:\n            n_generators (int): Number of sub-generators to be used in the patch method.\n            q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n        \"\"\"\n\n        super().__init__()\n\n        self.q_params = nn.ParameterList(\n            [\n                nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n                for _ in range(n_generators)\n            ]\n        )\n        self.n_generators = n_generators\n\n    def forward(self, x):\n        # Size of each sub-generator output\n        patch_size = 2 ** (n_qubits - n_a_qubits)\n\n        # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n        images = torch.Tensor(x.size(0), 0).to(device)\n\n        # Iterate over all sub-generators\n        for params in self.q_params:\n\n            # Create a Tensor to 'catch' a batch of the patches from a single sub-generator\n            patches = torch.Tensor(0, patch_size).to(device)\n            for elem in x:\n                q_out = partial_measure(elem, params).float().unsqueeze(0)\n                patches = torch.cat((patches, q_out))\n\n            # Each batch of patches is concatenated with each other to create a batch of images\n            images = torch.cat((images, patches), 1)\n\n        return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s define learning rates and number of iterations for the training\nprocess.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lrG = 0.3  # Learning rate for the generator\nlrD = 0.01  # Learning rate for the discriminator\nnum_iter = 500  # Number of training iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now putting everything together and executing the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator().to(device)\ngenerator = PatchQuantumGenerator(n_generators).to(device)\n\n# Binary cross entropy\ncriterion = nn.BCELoss()\n\n# Optimisers\noptD = optim.SGD(discriminator.parameters(), lr=lrD)\noptG = optim.SGD(generator.parameters(), lr=lrG)\n\nreal_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\nfake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n\n# Fixed noise allows us to visually track the generated images throughout training\nfixed_noise = torch.rand(8, n_qubits, device=device) * math.pi / 2\n\n# Iteration counter\ncounter = 0\n\n# Collect images for plotting later\nresults = []\n\nwhile True:\n    for i, (data, _) in enumerate(dataloader):\n\n        # Data for training the discriminator\n        data = data.reshape(-1, image_size * image_size)\n        real_data = data.to(device)\n\n        # Noise follwing a uniform distribution in range [0,pi/2)\n        noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n        fake_data = generator(noise)\n\n        # Training the discriminator\n        discriminator.zero_grad()\n        outD_real = discriminator(real_data).view(-1)\n        outD_fake = discriminator(fake_data.detach()).view(-1)\n\n        errD_real = criterion(outD_real, real_labels)\n        errD_fake = criterion(outD_fake, fake_labels)\n        # Propagate gradients\n        errD_real.backward()\n        errD_fake.backward()\n\n        errD = errD_real + errD_fake\n        optD.step()\n\n        # Training the generator\n        generator.zero_grad()\n        outD_fake = discriminator(fake_data).view(-1)\n        errG = criterion(outD_fake, real_labels)\n        errG.backward()\n        optG.step()\n\n        counter += 1\n\n        # Show loss values         \n        if counter % 10 == 0:\n            print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n            test_images = generator(fixed_noise).view(8,1,image_size,image_size).cpu().detach()\n            \n            # Save images every 50 iterations\n            if counter % 50 == 0:\n                results.append(test_images)\n\n        if counter == num_iter:\n            break\n    if counter == num_iter:\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.title}\nNote\n:::\n\nYou may have noticed `errG = criterion(outD_fake, real_labels)` and\nwondered why we don't use `fake_labels` instead of `real_labels`.\nHowever, this is simply a trick to be able to use the same `criterion`\nfunction for both the generator and discriminator. Using `real_labels`\nforces the generator loss function to use the $\\log(D(G(z))$ term\ninstead of the $\\log(1 - D(G(z))$ term of the binary cross entropy loss\nfunction.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot how the generated images evolved throughout training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 5))\nouter = gridspec.GridSpec(5, 2, wspace=0.1)\n\nfor i, images in enumerate(results):\n    inner = gridspec.GridSpecFromSubplotSpec(1, images.size(0),\n                    subplot_spec=outer[i])\n    \n    images = torch.squeeze(images, dim=1)\n    for j, im in enumerate(images):\n\n        ax = plt.Subplot(fig, inner[j])\n        ax.imshow(im.numpy(), cmap=\"gray\")\n        ax.set_xticks([])\n        ax.set_yticks([])\n        if j==0:\n            ax.set_title(f'Iteration {50+i*50}', loc='left')\n        fig.add_subplot(ax)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Acknowledgements\n================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Many thanks to Karolis \u0160pukas who I co-developed much of the code with.\nI also extend my thanks to Dr.\u00a0Yuxuan Du for answering my questions\nregarding his paper. I am also indebited to the Pennylane community for\ntheir help over the past few years.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n==========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\[1\\] Ian J. Goodfellow et al.\u00a0\\*Generative Adversarial Networks\\*.\n[arXiv:1406.2661](https://arxiv.org/abs/1406.2661) (2014).\n\n\\[2\\] He-Liang Huang et al.\u00a0\\*Experimental Quantum Generative\nAdversarial Networks for Image Generation\\*.\n[arXiv:2010.06201](https://arxiv.org/abs/2010.06201) (2020).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}