{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training and evaluating quantum kernels\n=======================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Kernels and alignment training with\nPennylane. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/QEK_thumbnail.png>\n:::\n\n::: {.related}\ntutorial\\_kernel\\_based\\_training Kernel-based training with\nscikit-learn tutorial\\_data\\_reuploading\\_classifier Classification with\ndata reuploading\n:::\n\n*Authors: Peter-Jan Derks, Paul F\u00e4hrmann, Elies Gil-Fuster, Tom\nHubregtsen, Johannes Jakob Meyer and David Wierichs. Posted: 24 June\n2021*\n\nKernel methods are one of the cornerstones of classical machine\nlearning. Here we are concerned with kernels that can be evaluated on\nquantum computers, *quantum kernels* for short. In this tutorial you\nwill learn how to evaluate kernels, use them for classification and\ntrain them with gradient-based optimization, and all that using the\nfunctionality of PennyLane\\'s [kernels\nmodule](https://pennylane.readthedocs.io/en/latest/code/qml_kernels.html).\nThe demo is based on Ref., a project from Xanadu\\'s own\n[QHack](https://qhack.ai/) hackathon.\n\nWhat are kernel methods?\n------------------------\n\nTo understand what a kernel method does, let\\'s first revisit one of the\nsimplest methods to assign binary labels to datapoints: linear\nclassification.\n\nImagine we want to discern two different classes of points that lie in\ndifferent corners of the plane. A linear classifier corresponds to\ndrawing a line and assigning different labels to the regions on opposing\nsides of the line:\n\n![](../demonstrations/kernels_module/linear_classification.png){.align-center\nwidth=\"30.0%\"}\n\nWe can mathematically formalize this by assigning the label $y$ via\n\n$$y(\\boldsymbol{x}) = \\operatorname{sgn}(\\langle \\boldsymbol{w}, \\boldsymbol{x}\\rangle + b).$$\n\nThe vector $\\boldsymbol{w}$ points perpendicular to the line and thus\ndetermine its slope. The independent term $b$ specifies the position on\nthe plane. In this form, linear classification can also be extended to\nhigher dimensional vectors $\\boldsymbol{x}$, where a line does not\ndivide the entire space into two regions anymore. Instead one needs a\n*hyperplane*. It is immediately clear that this method is not very\npowerful, as datasets that are not separable by a hyperplane can\\'t be\nclassified without error.\n\nWe can actually sneak around this limitation by performing a neat trick:\nif we define some map $\\phi(\\boldsymbol{x})$ that *embeds* our\ndatapoints into a larger *feature space* and then perform linear\nclassification there, we could actually realise non-linear\nclassification in our original space!\n\n![](../demonstrations/kernels_module/embedding_nonlinear_classification.png){.align-center\nwidth=\"65.0%\"}\n\nIf we go back to the expression for our prediction and include the\nembedding, we get\n\n$$y(\\boldsymbol{x}) = \\operatorname{sgn}(\\langle \\boldsymbol{w}, \\phi(\\boldsymbol{x})\\rangle + b).$$\n\nWe will forgo one tiny step, but it can be shown that for the purpose of\noptimal classification, we can choose the vector defining the decision\nboundary as a linear combination of the embedded datapoints\n$\\boldsymbol{w} = \\sum_i \\alpha_i \\phi(\\boldsymbol{x}_i)$. Putting this\ninto the formula yields\n\n$$y(\\boldsymbol{x}) = \\operatorname{sgn}\\left(\\sum_i \\alpha_i \\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x})\\rangle + b\\right).$$\n\nThis rewriting might not seem useful at first, but notice the above\nformula only contains inner products between vectors in the embedding\nspace:\n\n$$k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x}_j)\\rangle.$$\n\nWe call this function the *kernel*. It provides the advantage that we\ncan often find an explicit formula for the kernel $k$ that makes it\nsuperfluous to actually perform the (potentially expensive) embedding\n$\\phi$. Consider for example the following embedding and the associated\nkernel:\n\n$$\\begin{aligned}\n\\phi((x_1, x_2)) &= (x_1^2, \\sqrt{2} x_1 x_2, x_2^2) \\\\\nk(\\boldsymbol{x}, \\boldsymbol{y}) &= x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 = \\langle \\boldsymbol{x}, \\boldsymbol{y} \\rangle^2.\n\\end{aligned}$$\n\nThis means by just replacing the regular scalar product in our linear\nclassification with the map $k$, we can actually express much more\nintricate decision boundaries!\n\nThis is very important, because in many interesting cases the embedding\n$\\phi$ will be much costlier to compute than the kernel $k$.\n\nIn this demo, we will explore one particular kind of kernel that can be\nrealized on near-term quantum computers, namely *Quantum Embedding\nKernels (QEKs)*. These are kernels that arise from embedding data into\nthe space of quantum states. We formalize this by considering a\nparameterised quantum circuit $U(\\boldsymbol{x})$ that maps a datapoint\n$\\boldsymbol{x}$ to the state\n\n$$|\\psi(\\boldsymbol{x})\\rangle = U(\\boldsymbol{x}) |0 \\rangle.$$\n\nThe kernel value is then given by the *overlap* of the associated\nembedded quantum states\n\n$$k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = | \\langle\\psi(\\boldsymbol{x}_i)|\\psi(\\boldsymbol{x}_j)\\rangle|^2.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A toy problem\n=============\n\nIn this demo, we will treat a toy problem that showcases the inner\nworkings of classification with quantum embedding kernels, training\nvariational embedding kernels and the available functionalities to do\nboth in PennyLane. We of course need to start with some imports:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pennylane import numpy as np\nimport matplotlib as mpl\n\nnp.random.seed(1359)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we proceed right away to create a dataset to work with, the\n`DoubleCake` dataset. Firstly, we define two functions to enable us to\ngenerate the data. The details of these functions are not essential for\nunderstanding the demo, so don\\'t mind them if they are confusing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _make_circular_data(num_sectors):\n    \"\"\"Generate datapoints arranged in an even circle.\"\"\"\n    center_indices = np.array(range(0, num_sectors))\n    sector_angle = 2 * np.pi / num_sectors\n    angles = (center_indices + 0.5) * sector_angle\n    x = 0.7 * np.cos(angles)\n    y = 0.7 * np.sin(angles)\n    labels = 2 * np.remainder(np.floor_divide(angles, sector_angle), 2) - 1\n\n    return x, y, labels\n\n\ndef make_double_cake_data(num_sectors):\n    x1, y1, labels1 = _make_circular_data(num_sectors)\n    x2, y2, labels2 = _make_circular_data(num_sectors)\n\n    # x and y coordinates of the datapoints\n    x = np.hstack([x1, 0.5 * x2])\n    y = np.hstack([y1, 0.5 * y2])\n\n    # Canonical form of dataset\n    X = np.vstack([x, y]).T\n\n    labels = np.hstack([labels1, -1 * labels2])\n\n    # Canonical form of labels\n    Y = labels.astype(int)\n\n    return X, Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define a function to help plot the `DoubleCake` data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_double_cake_data(X, Y, ax, num_sectors=None):\n    \"\"\"Plot double cake data and corresponding sectors.\"\"\"\n    x, y = X.T\n    cmap = mpl.colors.ListedColormap([\"#FF0000\", \"#0000FF\"])\n    ax.scatter(x, y, c=Y, cmap=cmap, s=25, marker=\"s\")\n\n    if num_sectors is not None:\n        sector_angle = 360 / num_sectors\n        for i in range(num_sectors):\n            color = [\"#FF0000\", \"#0000FF\"][(i % 2)]\n            other_color = [\"#FF0000\", \"#0000FF\"][((i + 1) % 2)]\n            ax.add_artist(\n                mpl.patches.Wedge(\n                    (0, 0),\n                    1,\n                    i * sector_angle,\n                    (i + 1) * sector_angle,\n                    lw=0,\n                    color=color,\n                    alpha=0.1,\n                    width=0.5,\n                )\n            )\n            ax.add_artist(\n                mpl.patches.Wedge(\n                    (0, 0),\n                    0.5,\n                    i * sector_angle,\n                    (i + 1) * sector_angle,\n                    lw=0,\n                    color=other_color,\n                    alpha=0.1,\n                )\n            )\n            ax.set_xlim(-1, 1)\n\n    ax.set_ylim(-1, 1)\n    ax.set_aspect(\"equal\")\n    ax.axis(\"off\")\n\n    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s now have a look at our dataset. In our example, we will work with\n3 sectors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nnum_sectors = 3\nX, Y = make_double_cake_data(num_sectors)\n\nax = plot_double_cake_data(X, Y, plt.gca(), num_sectors=num_sectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining a Quantum Embedding Kernel\n===================================\n\nPennyLane\\'s [kernels\nmodule](https://pennylane.readthedocs.io/en/latest/code/qml_kernels.html)\nallows for a particularly simple implementation of Quantum Embedding\nKernels. The first ingredient we need for this is an *ansatz*, which we\nwill construct by repeating a layer as building block. Let\\'s start by\ndefining this layer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n\n\ndef layer(x, params, wires, i0=0, inc=1):\n    \"\"\"Building block of the embedding ansatz\"\"\"\n    i = i0\n    for j, wire in enumerate(wires):\n        qml.Hadamard(wires=[wire])\n        qml.RZ(x[i % len(x)], wires=[wire])\n        i += inc\n        qml.RY(params[0, j], wires=[wire])\n\n    qml.broadcast(unitary=qml.CRZ, pattern=\"ring\", wires=wires, parameters=params[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct the ansatz, this layer is repeated multiple times, reusing\nthe datapoint `x` but feeding different variational parameters `params`\ninto each of them. Together, the datapoint and the variational\nparameters fully determine the embedding ansatz $U(\\boldsymbol{x})$. In\norder to construct the full kernel circuit, we also require its adjoint\n$U(\\boldsymbol{x})^\\dagger$, which we can obtain via `qml.adjoint`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ansatz(x, params, wires):\n    \"\"\"The embedding ansatz\"\"\"\n    for j, layer_params in enumerate(params):\n        layer(x, layer_params, wires, i0=j * len(wires))\n\n\nadjoint_ansatz = qml.adjoint(ansatz)\n\n\ndef random_params(num_wires, num_layers):\n    \"\"\"Generate random variational parameters in the shape for the ansatz.\"\"\"\n    return np.random.uniform(0, 2 * np.pi, (num_layers, 2, num_wires), requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Together with the ansatz we only need a device to run the quantum\ncircuit on. For the purpose of this tutorial we will use PennyLane\\'s\n`default.qubit` device with 5 wires in analytic mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=5, shots=None)\nwires = dev.wires.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now define the quantum circuit that realizes the kernel. We will\ncompute the overlap of the quantum states by first applying the\nembedding of the first datapoint and then the adjoint of the embedding\nof the second datapoint. We finally extract the probabilities of\nobserving each basis state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef kernel_circuit(x1, x2, params):\n    ansatz(x1, params, wires=wires)\n    adjoint_ansatz(x2, params, wires=wires)\n    return qml.probs(wires=wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The kernel function itself is now obtained by looking at the probability\nof observing the all-zero state at the end of the kernel circuit \\--\nbecause of the ordering in `qml.probs`, this is the first entry:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def kernel(x1, x2, params):\n    return kernel_circuit(x1, x2, params)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.title}\nNote\n:::\n\nAn alternative way to set up the kernel circuit in PennyLane would be to\nuse the observable type\n[Projector](https://pennylane.readthedocs.io/en/latest/code/api/pennylane.Projector.html).\nThis is shown in the [demo on kernel-based training of quantum\nmodels](https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html),\nwhere you will also find more background information on the kernel\ncircuit structure itself.\n:::\n\nBefore focusing on the kernel values we have to provide values for the\nvariational parameters. At this point we fix the number of layers in the\nansatz circuit to $6$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_params = random_params(num_wires=5, num_layers=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can have a look at the kernel value between the first and the\nsecond datapoint:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel_value = kernel(X[0], X[1], init_params)\nprint(f\"The kernel value between the first and second datapoint is {kernel_value:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mutual kernel values between all elements of the dataset form the\n*kernel matrix*. We can inspect it via the\n`qml.kernels.square_kernel_matrix` method, which makes use of symmetry\nof the kernel,\n$k(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = k(\\boldsymbol{x}_j, \\boldsymbol{x}_i)$.\nIn addition, the option `assume_normalized_kernel=True` ensures that we\ndo not calculate the entries between the same datapoints, as we know\nthem to be 1 for our noiseless simulation. Overall this means that we\ncompute $\\frac{1}{2}(N^2-N)$ kernel values for $N$ datapoints. To\ninclude the variational parameters, we construct a `lambda` function\nthat fixes them to the values we sampled above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_kernel = lambda x1, x2: kernel(x1, x2, init_params)\nK_init = qml.kernels.square_kernel_matrix(X, init_kernel, assume_normalized_kernel=True)\n\nwith np.printoptions(precision=3, suppress=True):\n    print(K_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the Quantum Embedding Kernel for predictions\n==================================================\n\nThe quantum kernel alone can not be used to make predictions on a\ndataset, becaues it is essentially just a tool to measure the similarity\nbetween two datapoints. To perform an actual prediction we will make use\nof scikit-learn\\'s Support Vector Classifier (SVC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To construct the SVM, we need to supply `sklearn.svm.SVC` with a\nfunction that takes two sets of datapoints and returns the associated\nkernel matrix. We can make use of the function\n`qml.kernels.kernel_matrix` that provides this functionality. It expects\nthe kernel to not have additional parameters besides the datapoints,\nwhich is why we again supply the variational parameters via the `lambda`\nfunction from above. Once we have this, we can let scikit-learn adjust\nthe SVM from our Quantum Embedding Kernel.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThis step does *not* modify the variational parameters in our circuit\nansatz. What it does is solving a different optimization task for the\n$\\alpha$ and $b$ vectors we introduced in the beginning.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel=lambda X1, X2: qml.kernels.kernel_matrix(X1, X2, init_kernel)).fit(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how well our classifier performs we will measure which percentage\nof the dataset it classifies correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def accuracy(classifier, X, Y_target):\n    return 1 - np.count_nonzero(classifier.predict(X) - Y_target) / len(Y_target)\n\n\naccuracy_init = accuracy(svm, X, Y)\nprint(f\"The accuracy of the kernel with random parameters is {accuracy_init:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are also interested in seeing what the decision boundaries in this\nclassification look like. This could help us spotting overfitting issues\nvisually in more complex data sets. To this end we will introduce a\nsecond helper method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundaries(classifier, ax, N_gridpoints=14):\n    _xx, _yy = np.meshgrid(np.linspace(-1, 1, N_gridpoints), np.linspace(-1, 1, N_gridpoints))\n\n    _zz = np.zeros_like(_xx)\n    for idx in np.ndindex(*_xx.shape):\n        _zz[idx] = classifier.predict(np.array([_xx[idx], _yy[idx]])[np.newaxis, :])\n\n    plot_data = {\"_xx\": _xx, \"_yy\": _yy, \"_zz\": _zz}\n    ax.contourf(\n        _xx,\n        _yy,\n        _zz,\n        cmap=mpl.colors.ListedColormap([\"#FF0000\", \"#0000FF\"]),\n        alpha=0.2,\n        levels=[-1, 0, 1],\n    )\n    plot_double_cake_data(X, Y, ax)\n\n    return plot_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With that done, let\\'s have a look at the decision boundaries for our\ninitial classifier:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_plot_data = plot_decision_boundaries(svm, plt.gca())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see the outer points in the dataset can be correctly classified, but\nwe still struggle with the inner circle. But remember we have a circuit\nwith many free parameters! It is reasonable to believe we can give\nvalues to those variational parameters which improve the overall\naccuracy of our SVC.\n\nTraining the Quantum Embedding Kernel\n=====================================\n\nTo be able to train the Quantum Embedding Kernel we need some measure of\nhow well it fits the dataset in question. Performing an exhaustive\nsearch in parameter space is not a good solution because it is very\nresource intensive, and since the accuracy is a discrete quantity we\nwould not be able to detect small improvements.\n\nWe can, however, resort to a more specialized measure, the\n*kernel-target alignment*. The kernel-target alignment compares the\nsimilarity predicted by the quantum kernel to the actual labels of the\ntraining data. It is based on *kernel alignment*, a similiarity measure\nbetween two kernels with given kernel matrices $K_1$ and $K_2$:\n\n$$\\operatorname{KA}(K_1, K_2) = \\frac{\\operatorname{Tr}(K_1 K_2)}{\\sqrt{\\operatorname{Tr}(K_1^2)\\operatorname{Tr}(K_2^2)}}.$$\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nSeen from a more theoretical side, $\\operatorname{KA}$ is nothing else\nthan the cosine of the angle between the kernel matrices $K_1$ and $K_2$\nif we see them as vectors in the space of matrices with the\nHilbert-Schmidt (or Frobenius) scalar product\n$\\langle A, B \\rangle = \\operatorname{Tr}(A^T B)$. This reinforces the\ngeometric picture of how this measure relates to objects, namely two\nkernels, being aligned in a vector space.\n:::\n\nThe training data enters the picture by defining an *ideal* kernel\nfunction that expresses the original labelling in the vector\n$\\boldsymbol{y}$ by assigning to two datapoints the product of the\ncorresponding labels:\n\n$$k_{\\boldsymbol{y}}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = y_i y_j.$$\n\nThe assigned kernel is thus $+1$ if both datapoints lie in the same\nclass and $-1$ otherwise and its kernel matrix is simply given by the\nouter product $\\boldsymbol{y}\\boldsymbol{y}^T$. The kernel-target\nalignment is then defined as the kernel alignment of the kernel matrix\n$K$ generated by the quantum kernel and\n$\\boldsymbol{y}\\boldsymbol{y}^T$:\n\n$$\\operatorname{KTA}_{\\boldsymbol{y}}(K)\n= \\frac{\\operatorname{Tr}(K \\boldsymbol{y}\\boldsymbol{y}^T)}{\\sqrt{\\operatorname{Tr}(K^2)\\operatorname{Tr}((\\boldsymbol{y}\\boldsymbol{y}^T)^2)}}\n= \\frac{\\boldsymbol{y}^T K \\boldsymbol{y}}{\\sqrt{\\operatorname{Tr}(K^2)} N}$$\n\nwhere $N$ is the number of elements in $\\boldsymbol{y}$, that is the\nnumber of datapoints in the dataset.\n\nIn summary, the kernel-target alignment effectively captures how well\nthe kernel you chose reproduces the actual similarities of the data. It\ndoes have one drawback, however: having a high kernel-target alignment\nis only a necessary but not a sufficient condition for a good\nperformance of the kernel. This means having good alignment is\nguaranteed for good performance, but optimal alignment will not always\nbring optimal training accuracy with it.\n\nLet\\'s now come back to the actual implementation. PennyLane\\'s\n`kernels` module allows you to easily evaluate the kernel target\nalignment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kta_init = qml.kernels.target_alignment(X, Y, init_kernel, assume_normalized_kernel=True)\n\nprint(f\"The kernel-target alignment for our dataset and random parameters is {kta_init:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let\\'s code up an optimization loop and improve the kernel-target\nalignment!\n\nWe will make use of regular gradient descent optimization. To speed up\nthe optimization we will not use the entire training set to compute\n$\\operatorname{KTA}$ but rather sample smaller subsets of the data at\neach step, we choose $4$ datapoints at random. Remember that\nPennyLane\\'s built-in optimizer works to *minimize* the cost function\nthat is given to it, which is why we have to multiply the kernel target\nalignment by $-1$ to actually *maximize* it in the process.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nCurrently, the function `qml.kernels.target_alignment` is not\ndifferentiable yet, making it unfit for gradient descent optimization.\nWe therefore first define a differentiable version of this function.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def target_alignment(\n    X,\n    Y,\n    kernel,\n    assume_normalized_kernel=False,\n    rescale_class_labels=True,\n):\n    \"\"\"Kernel-target alignment between kernel and labels.\"\"\"\n\n    K = qml.kernels.square_kernel_matrix(\n        X,\n        kernel,\n        assume_normalized_kernel=assume_normalized_kernel,\n    )\n\n    if rescale_class_labels:\n        nplus = np.count_nonzero(np.array(Y) == 1)\n        nminus = len(Y) - nplus\n        _Y = np.array([y / nplus if y == 1 else y / nminus for y in Y])\n    else:\n        _Y = np.array(Y)\n\n    T = np.outer(_Y, _Y)\n    inner_product = np.sum(K * T)\n    norm = np.sqrt(np.sum(K * K) * np.sum(T * T))\n    inner_product = inner_product / norm\n\n    return inner_product\n\n\nparams = init_params\nopt = qml.GradientDescentOptimizer(0.2)\n\nfor i in range(500):\n    # Choose subset of datapoints to compute the KTA on.\n    subset = np.random.choice(list(range(len(X))), 4)\n    # Define the cost function for optimization\n    cost = lambda _params: -target_alignment(\n        X[subset],\n        Y[subset],\n        lambda x1, x2: kernel(x1, x2, _params),\n        assume_normalized_kernel=True,\n    )\n    # Optimization step\n    params = opt.step(cost, params)\n\n    # Report the alignment on the full dataset every 50 steps.\n    if (i + 1) % 50 == 0:\n        current_alignment = target_alignment(\n            X,\n            Y,\n            lambda x1, x2: kernel(x1, x2, params),\n            assume_normalized_kernel=True,\n        )\n        print(f\"Step {i+1} - Alignment = {current_alignment:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to assess the impact of training the parameters of the quantum\nkernel. Thus, let\\'s build a second support vector classifier with the\ntrained kernel:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First create a kernel with the trained parameter baked into it.\ntrained_kernel = lambda x1, x2: kernel(x1, x2, params)\n\n# Second create a kernel matrix function using the trained kernel.\ntrained_kernel_matrix = lambda X1, X2: qml.kernels.kernel_matrix(X1, X2, trained_kernel)\n\n# Note that SVC expects the kernel argument to be a kernel matrix function.\nsvm_trained = SVC(kernel=trained_kernel_matrix).fit(X, Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We expect to see an accuracy improvement vs.\u00a0the SVM with random\nparameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy_trained = accuracy(svm_trained, X, Y)\nprint(f\"The accuracy of a kernel with trained parameters is {accuracy_trained:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have now achieved perfect classification! \ud83c\udf86\n\nFollowing on the results that SVM\\'s have proven good generalisation\nbehavior, it will be interesting to inspect the decision boundaries of\nour classifier:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trained_plot_data = plot_decision_boundaries(svm_trained, plt.gca())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indeed, we see that now not only every data instance falls within the\ncorrect class, but also that there are no strong artifacts that would\nmake us distrust the model. In this sense, our approach benefits from\nboth: on one hand it can adjust itself to the dataset, and on the other\nhand is not expected to suffer from bad generalisation.\n\nReferences\n==========\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}