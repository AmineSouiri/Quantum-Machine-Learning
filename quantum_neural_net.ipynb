{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function fitting with a photonic quantum neural network {#quantum_neural_net}\n=======================================================\n\n::: {.meta}\n:property=\\\"og:description\\\": Fit to noisy data with a variational\nquantum circuit. :property=\\\"og:image\\\":\n<https://pennylane.ai/qml/_images/qnn_output_28_0.png>\n:::\n\n::: {.related}\nqonn Optimizing a quantum optical neural network pytorch\\_noise PyTorch\nand noisy devices tutorial\\_noisy\\_circuit\\_optimization Optimizing\nnoisy circuits with Cirq\n:::\n\n*Author: PennyLane dev team. Last updated: 25 Jan 2021.*\n\nIn this example we show how a variational circuit can be used to learn a\nfit for a one-dimensional function when being trained with noisy samples\nfrom that function.\n\nThe variational circuit we use is the continuous-variable quantum neural\nnetwork model described in [Killoran et al.\n(2018)](https://arxiv.org/abs/1806.06871).\n\nImports\n-------\n\nWe import PennyLane, the wrapped version of NumPy provided by PennyLane,\nand an optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.optimize import AdamOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The device we use is the Strawberry Fields simulator, this time with\nonly one quantum mode (or `wire`). You will need to have the Strawberry\nFields plugin for PennyLane installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum node\n============\n\nFor a single quantum mode, each layer of the variational circuit is\ndefined as:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(v):\n    # Matrix multiplication of input layer\n    qml.Rotation(v[0], wires=0)\n    qml.Squeezing(v[1], 0.0, wires=0)\n    qml.Rotation(v[2], wires=0)\n\n    # Bias\n    qml.Displacement(v[3], 0.0, wires=0)\n\n    # Element-wise nonlinear transformation\n    qml.Kerr(v[4], wires=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variational circuit in the quantum node first encodes the input into\nthe displacement of the mode, and then executes the layers. The output\nis the expectation of the x-quadrature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef quantum_neural_net(var, x):\n    # Encode input x into quantum state\n    qml.Displacement(x, 0.0, wires=0)\n\n    # \"layer\" subcircuits\n    for v in var:\n        layer(v)\n\n    return qml.expval(qml.X(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objective\n=========\n\nAs an objective we take the square loss between target labels and model\npredictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def square_loss(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        loss = loss + (l - p) ** 2\n\n    loss = loss / len(labels)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cost function, we compute the outputs from the variational\ncircuit. Function fitting is a regression problem, and we interpret the\nexpectations from the quantum node as predictions (i.e., without\napplying postprocessing such as thresholding).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(var, features, labels):\n    preds = [quantum_neural_net(var, x) for x in features]\n    return square_loss(labels, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n============\n\nWe load noisy data samples of a sine function from the external file\n`sine.txt`\n(`<a href=\"https://raw.githubusercontent.com/XanaduAI/pennylane/v0.3.0/examples/data/sine.txt\"\ndownload=\"sine.txt\" target=\"_blank\">download the file here</a>`{.interpreted-text\nrole=\"html\"}).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"sine.txt\")\nX = np.array(data[:, 0], requires_grad=False)\nY = np.array(data[:, 1], requires_grad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before training a model, let\\'s examine the data.\n\n*Note: For the next cell to work you need the matplotlib library.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(X, Y)\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"f(x)\", fontsize=18)\nplt.tick_params(axis=\"both\", which=\"major\", labelsize=16)\nplt.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](../demonstrations/quantum_neural_net/qnn_output_20_0.png)\n\nThe network's weights (called `var` here) are initialized with values\nsampled from a normal distribution. We use 4 layers; performance has\nbeen found to plateau at around 6 layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nnum_layers = 4\nvar_init = 0.05 * np.random.randn(num_layers, 5, requires_grad=True)\nprint(var_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.rst-class}\nsphx-glr-script-out\n\nOut:\n\n``` {.none}\narray([[ 0.08820262,  0.02000786,  0.0489369 ,  0.11204466,  0.0933779 ],\n       [-0.04886389,  0.04750442, -0.00756786, -0.00516094,  0.02052993],\n       [ 0.00720218,  0.07271368,  0.03805189,  0.00608375,  0.02219316],\n       [ 0.01668372,  0.07470395, -0.01025791,  0.01565339, -0.04270479]])\n```\n:::\n\nUsing the Adam optimizer, we update the weights for 500 steps (this\ntakes some time). More steps will lead to a better fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = AdamOptimizer(0.01, beta1=0.9, beta2=0.999)\n\nvar = var_init\nfor it in range(500):\n    (var, _, _), _cost = opt.step_and_cost(cost, var, X, Y)\n    print(\"Iter: {:5d} | Cost: {:0.7f} \".format(it, _cost))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.rst-class}\nsphx-glr-script-out\n\nOut:\n\n``` {.none}\nIter:     0 | Cost: 0.3006065\nIter:     1 | Cost: 0.2689702\nIter:     2 | Cost: 0.2472125\nIter:     3 | Cost: 0.2300139\nIter:     4 | Cost: 0.2157100\nIter:     5 | Cost: 0.2035455\nIter:     6 | Cost: 0.1931103\nIter:     7 | Cost: 0.1841536\nIter:     8 | Cost: 0.1765061\nIter:     9 | Cost: 0.1700410\nIter:    10 | Cost: 0.1646527\nIter:    11 | Cost: 0.1602444\nIter:    12 | Cost: 0.1567201\nIter:    13 | Cost: 0.1539806\nIter:    14 | Cost: 0.1519220\nIter:    15 | Cost: 0.1504356\nIter:    16 | Cost: 0.1494099\nIter:    17 | Cost: 0.1487330\nIter:    18 | Cost: 0.1482962\nIter:    19 | Cost: 0.1479980\nIter:    20 | Cost: 0.1477470\nIter:    21 | Cost: 0.1474655\nIter:    22 | Cost: 0.1470914\nIter:    23 | Cost: 0.1465799\nIter:    24 | Cost: 0.1459034\nIter:    25 | Cost: 0.1450506\nIter:    26 | Cost: 0.1440251\nIter:    27 | Cost: 0.1428427\nIter:    28 | Cost: 0.1415282\nIter:    29 | Cost: 0.1401125\nIter:    30 | Cost: 0.1386296\nIter:    31 | Cost: 0.1371132\nIter:    32 | Cost: 0.1355946\nIter:    33 | Cost: 0.1341006\nIter:    34 | Cost: 0.1326526\nIter:    35 | Cost: 0.1312654\nIter:    36 | Cost: 0.1299478\nIter:    37 | Cost: 0.1287022\nIter:    38 | Cost: 0.1275259\nIter:    39 | Cost: 0.1264120\nIter:    40 | Cost: 0.1253502\nIter:    41 | Cost: 0.1243284\nIter:    42 | Cost: 0.1233333\nIter:    43 | Cost: 0.1223521\nIter:    44 | Cost: 0.1213726\nIter:    45 | Cost: 0.1203843\nIter:    46 | Cost: 0.1193790\nIter:    47 | Cost: 0.1183506\nIter:    48 | Cost: 0.1172959\nIter:    49 | Cost: 0.1162138\nIter:    50 | Cost: 0.1151057\nIter:    51 | Cost: 0.1139748\nIter:    52 | Cost: 0.1128259\nIter:    53 | Cost: 0.1116647\nIter:    54 | Cost: 0.1104972\nIter:    55 | Cost: 0.1093295\nIter:    56 | Cost: 0.1081673\nIter:    57 | Cost: 0.1070151\nIter:    58 | Cost: 0.1058764\nIter:    59 | Cost: 0.1047533\nIter:    60 | Cost: 0.1036464\nIter:    61 | Cost: 0.1025554\nIter:    62 | Cost: 0.1014787\nIter:    63 | Cost: 0.1004141\nIter:    64 | Cost: 0.0993591\nIter:    65 | Cost: 0.0983111\nIter:    66 | Cost: 0.0972679\nIter:    67 | Cost: 0.0962278\nIter:    68 | Cost: 0.0951896\nIter:    69 | Cost: 0.0941534\nIter:    70 | Cost: 0.0931195\nIter:    71 | Cost: 0.0920891\nIter:    72 | Cost: 0.0910638\nIter:    73 | Cost: 0.0900453\nIter:    74 | Cost: 0.0890357\nIter:    75 | Cost: 0.0880366\nIter:    76 | Cost: 0.0870493\nIter:    77 | Cost: 0.0860751\nIter:    78 | Cost: 0.0851144\nIter:    79 | Cost: 0.0841675\nIter:    80 | Cost: 0.0832342\nIter:    81 | Cost: 0.0823143\nIter:    82 | Cost: 0.0814072\nIter:    83 | Cost: 0.0805125\nIter:    84 | Cost: 0.0796296\nIter:    85 | Cost: 0.0787583\nIter:    86 | Cost: 0.0778983\nIter:    87 | Cost: 0.0770497\nIter:    88 | Cost: 0.0762127\nIter:    89 | Cost: 0.0753874\nIter:    90 | Cost: 0.0745742\nIter:    91 | Cost: 0.0737733\nIter:    92 | Cost: 0.0729849\nIter:    93 | Cost: 0.0722092\nIter:    94 | Cost: 0.0714462\nIter:    95 | Cost: 0.0706958\nIter:    96 | Cost: 0.0699578\nIter:    97 | Cost: 0.0692319\nIter:    98 | Cost: 0.0685177\nIter:    99 | Cost: 0.0678151\nIter:   100 | Cost: 0.0671236\nIter:   101 | Cost: 0.0664430\nIter:   102 | Cost: 0.0657732\nIter:   103 | Cost: 0.0651139\nIter:   104 | Cost: 0.0644650\nIter:   105 | Cost: 0.0638264\nIter:   106 | Cost: 0.0631981\nIter:   107 | Cost: 0.0625800\nIter:   108 | Cost: 0.0619719\nIter:   109 | Cost: 0.0613737\nIter:   110 | Cost: 0.0607853\nIter:   111 | Cost: 0.0602064\nIter:   112 | Cost: 0.0596368\nIter:   113 | Cost: 0.0590764\nIter:   114 | Cost: 0.0585249\nIter:   115 | Cost: 0.0579820\nIter:   116 | Cost: 0.0574476\nIter:   117 | Cost: 0.0569214\nIter:   118 | Cost: 0.0564033\nIter:   119 | Cost: 0.0558932\nIter:   120 | Cost: 0.0553908\nIter:   121 | Cost: 0.0548960\nIter:   122 | Cost: 0.0544086\nIter:   123 | Cost: 0.0539286\nIter:   124 | Cost: 0.0534557\nIter:   125 | Cost: 0.0529897\nIter:   126 | Cost: 0.0525306\nIter:   127 | Cost: 0.0520781\nIter:   128 | Cost: 0.0516320\nIter:   129 | Cost: 0.0511923\nIter:   130 | Cost: 0.0507587\nIter:   131 | Cost: 0.0503311\nIter:   132 | Cost: 0.0499094\nIter:   133 | Cost: 0.0494934\nIter:   134 | Cost: 0.0490830\nIter:   135 | Cost: 0.0486781\nIter:   136 | Cost: 0.0482785\nIter:   137 | Cost: 0.0478842\nIter:   138 | Cost: 0.0474949\nIter:   139 | Cost: 0.0471107\nIter:   140 | Cost: 0.0467313\nIter:   141 | Cost: 0.0463567\nIter:   142 | Cost: 0.0459868\nIter:   143 | Cost: 0.0456214\nIter:   144 | Cost: 0.0452604\nIter:   145 | Cost: 0.0449038\nIter:   146 | Cost: 0.0445514\nIter:   147 | Cost: 0.0442032\nIter:   148 | Cost: 0.0438590\nIter:   149 | Cost: 0.0435188\nIter:   150 | Cost: 0.0431825\nIter:   151 | Cost: 0.0428499\nIter:   152 | Cost: 0.0425211\nIter:   153 | Cost: 0.0421960\nIter:   154 | Cost: 0.0418744\nIter:   155 | Cost: 0.0415563\nIter:   156 | Cost: 0.0412416\nIter:   157 | Cost: 0.0409302\nIter:   158 | Cost: 0.0406222\nIter:   159 | Cost: 0.0403173\nIter:   160 | Cost: 0.0400156\nIter:   161 | Cost: 0.0397169\nIter:   162 | Cost: 0.0394213\nIter:   163 | Cost: 0.0391286\nIter:   164 | Cost: 0.0388389\nIter:   165 | Cost: 0.0385520\nIter:   166 | Cost: 0.0382679\nIter:   167 | Cost: 0.0379866\nIter:   168 | Cost: 0.0377079\nIter:   169 | Cost: 0.0374319\nIter:   170 | Cost: 0.0371585\nIter:   171 | Cost: 0.0368877\nIter:   172 | Cost: 0.0366194\nIter:   173 | Cost: 0.0363535\nIter:   174 | Cost: 0.0360901\nIter:   175 | Cost: 0.0358291\nIter:   176 | Cost: 0.0355704\nIter:   177 | Cost: 0.0353140\nIter:   178 | Cost: 0.0350599\nIter:   179 | Cost: 0.0348081\nIter:   180 | Cost: 0.0345585\nIter:   181 | Cost: 0.0343110\nIter:   182 | Cost: 0.0340658\nIter:   183 | Cost: 0.0338226\nIter:   184 | Cost: 0.0335815\nIter:   185 | Cost: 0.0333425\nIter:   186 | Cost: 0.0331056\nIter:   187 | Cost: 0.0328706\nIter:   188 | Cost: 0.0326377\nIter:   189 | Cost: 0.0324067\nIter:   190 | Cost: 0.0321777\nIter:   191 | Cost: 0.0319506\nIter:   192 | Cost: 0.0317255\nIter:   193 | Cost: 0.0315022\nIter:   194 | Cost: 0.0312808\nIter:   195 | Cost: 0.0310613\nIter:   196 | Cost: 0.0308436\nIter:   197 | Cost: 0.0306278\nIter:   198 | Cost: 0.0304138\nIter:   199 | Cost: 0.0302016\nIter:   200 | Cost: 0.0299912\nIter:   201 | Cost: 0.0297826\nIter:   202 | Cost: 0.0295757\nIter:   203 | Cost: 0.0293707\nIter:   204 | Cost: 0.0291674\nIter:   205 | Cost: 0.0289659\nIter:   206 | Cost: 0.0287661\nIter:   207 | Cost: 0.0285681\nIter:   208 | Cost: 0.0283718\nIter:   209 | Cost: 0.0281772\nIter:   210 | Cost: 0.0279844\nIter:   211 | Cost: 0.0277933\nIter:   212 | Cost: 0.0276039\nIter:   213 | Cost: 0.0274163\nIter:   214 | Cost: 0.0272304\nIter:   215 | Cost: 0.0270461\nIter:   216 | Cost: 0.0268636\nIter:   217 | Cost: 0.0266829\nIter:   218 | Cost: 0.0265038\nIter:   219 | Cost: 0.0263264\nIter:   220 | Cost: 0.0261508\nIter:   221 | Cost: 0.0259768\nIter:   222 | Cost: 0.0258046\nIter:   223 | Cost: 0.0256341\nIter:   224 | Cost: 0.0254652\nIter:   225 | Cost: 0.0252981\nIter:   226 | Cost: 0.0251327\nIter:   227 | Cost: 0.0249690\nIter:   228 | Cost: 0.0248070\nIter:   229 | Cost: 0.0246467\nIter:   230 | Cost: 0.0244881\nIter:   231 | Cost: 0.0243312\nIter:   232 | Cost: 0.0241760\nIter:   233 | Cost: 0.0240225\nIter:   234 | Cost: 0.0238707\nIter:   235 | Cost: 0.0237206\nIter:   236 | Cost: 0.0235721\nIter:   237 | Cost: 0.0234254\nIter:   238 | Cost: 0.0232803\nIter:   239 | Cost: 0.0231369\nIter:   240 | Cost: 0.0229952\nIter:   241 | Cost: 0.0228552\nIter:   242 | Cost: 0.0227168\nIter:   243 | Cost: 0.0225801\nIter:   244 | Cost: 0.0224450\nIter:   245 | Cost: 0.0223116\nIter:   246 | Cost: 0.0221798\nIter:   247 | Cost: 0.0220496\nIter:   248 | Cost: 0.0219211\nIter:   249 | Cost: 0.0217942\nIter:   250 | Cost: 0.0216688\nIter:   251 | Cost: 0.0215451\nIter:   252 | Cost: 0.0214230\nIter:   253 | Cost: 0.0213024\nIter:   254 | Cost: 0.0211835\nIter:   255 | Cost: 0.0210660\nIter:   256 | Cost: 0.0209502\nIter:   257 | Cost: 0.0208358\nIter:   258 | Cost: 0.0207230\nIter:   259 | Cost: 0.0206117\nIter:   260 | Cost: 0.0205019\nIter:   261 | Cost: 0.0203936\nIter:   262 | Cost: 0.0202867\nIter:   263 | Cost: 0.0201813\nIter:   264 | Cost: 0.0200773\nIter:   265 | Cost: 0.0199748\nIter:   266 | Cost: 0.0198737\nIter:   267 | Cost: 0.0197740\nIter:   268 | Cost: 0.0196757\nIter:   269 | Cost: 0.0195787\nIter:   270 | Cost: 0.0194831\nIter:   271 | Cost: 0.0193889\nIter:   272 | Cost: 0.0192959\nIter:   273 | Cost: 0.0192043\nIter:   274 | Cost: 0.0191140\nIter:   275 | Cost: 0.0190249\nIter:   276 | Cost: 0.0189371\nIter:   277 | Cost: 0.0188505\nIter:   278 | Cost: 0.0187651\nIter:   279 | Cost: 0.0186810\nIter:   280 | Cost: 0.0185980\nIter:   281 | Cost: 0.0185163\nIter:   282 | Cost: 0.0184356\nIter:   283 | Cost: 0.0183561\nIter:   284 | Cost: 0.0182777\nIter:   285 | Cost: 0.0182004\nIter:   286 | Cost: 0.0181242\nIter:   287 | Cost: 0.0180491\nIter:   288 | Cost: 0.0179750\nIter:   289 | Cost: 0.0179020\nIter:   290 | Cost: 0.0178299\nIter:   291 | Cost: 0.0177589\nIter:   292 | Cost: 0.0176888\nIter:   293 | Cost: 0.0176197\nIter:   294 | Cost: 0.0175515\nIter:   295 | Cost: 0.0174843\nIter:   296 | Cost: 0.0174180\nIter:   297 | Cost: 0.0173525\nIter:   298 | Cost: 0.0172880\nIter:   299 | Cost: 0.0172243\nIter:   300 | Cost: 0.0171614\nIter:   301 | Cost: 0.0170994\nIter:   302 | Cost: 0.0170382\nIter:   303 | Cost: 0.0169777\nIter:   304 | Cost: 0.0169181\nIter:   305 | Cost: 0.0168592\nIter:   306 | Cost: 0.0168010\nIter:   307 | Cost: 0.0167436\nIter:   308 | Cost: 0.0166869\nIter:   309 | Cost: 0.0166309\nIter:   310 | Cost: 0.0165756\nIter:   311 | Cost: 0.0165209\nIter:   312 | Cost: 0.0164669\nIter:   313 | Cost: 0.0164136\nIter:   314 | Cost: 0.0163608\nIter:   315 | Cost: 0.0163087\nIter:   316 | Cost: 0.0162572\nIter:   317 | Cost: 0.0162063\nIter:   318 | Cost: 0.0161559\nIter:   319 | Cost: 0.0161061\nIter:   320 | Cost: 0.0160568\nIter:   321 | Cost: 0.0160080\nIter:   322 | Cost: 0.0159598\nIter:   323 | Cost: 0.0159121\nIter:   324 | Cost: 0.0158649\nIter:   325 | Cost: 0.0158181\nIter:   326 | Cost: 0.0157719\nIter:   327 | Cost: 0.0157260\nIter:   328 | Cost: 0.0156807\nIter:   329 | Cost: 0.0156357\nIter:   330 | Cost: 0.0155912\nIter:   331 | Cost: 0.0155471\nIter:   332 | Cost: 0.0155034\nIter:   333 | Cost: 0.0154601\nIter:   334 | Cost: 0.0154172\nIter:   335 | Cost: 0.0153747\nIter:   336 | Cost: 0.0153325\nIter:   337 | Cost: 0.0152907\nIter:   338 | Cost: 0.0152492\nIter:   339 | Cost: 0.0152081\nIter:   340 | Cost: 0.0151673\nIter:   341 | Cost: 0.0151269\nIter:   342 | Cost: 0.0150867\nIter:   343 | Cost: 0.0150469\nIter:   344 | Cost: 0.0150073\nIter:   345 | Cost: 0.0149681\nIter:   346 | Cost: 0.0149291\nIter:   347 | Cost: 0.0148905\nIter:   348 | Cost: 0.0148521\nIter:   349 | Cost: 0.0148140\nIter:   350 | Cost: 0.0147761\nIter:   351 | Cost: 0.0147385\nIter:   352 | Cost: 0.0147012\nIter:   353 | Cost: 0.0146641\nIter:   354 | Cost: 0.0146273\nIter:   355 | Cost: 0.0145907\nIter:   356 | Cost: 0.0145543\nIter:   357 | Cost: 0.0145182\nIter:   358 | Cost: 0.0144824\nIter:   359 | Cost: 0.0144467\nIter:   360 | Cost: 0.0144113\nIter:   361 | Cost: 0.0143762\nIter:   362 | Cost: 0.0143412\nIter:   363 | Cost: 0.0143065\nIter:   364 | Cost: 0.0142720\nIter:   365 | Cost: 0.0142378\nIter:   366 | Cost: 0.0142037\nIter:   367 | Cost: 0.0141699\nIter:   368 | Cost: 0.0141363\nIter:   369 | Cost: 0.0141030\nIter:   370 | Cost: 0.0140699\nIter:   371 | Cost: 0.0140370\nIter:   372 | Cost: 0.0140043\nIter:   373 | Cost: 0.0139719\nIter:   374 | Cost: 0.0139397\nIter:   375 | Cost: 0.0139077\nIter:   376 | Cost: 0.0138760\nIter:   377 | Cost: 0.0138445\nIter:   378 | Cost: 0.0138132\nIter:   379 | Cost: 0.0137822\nIter:   380 | Cost: 0.0137515\nIter:   381 | Cost: 0.0137210\nIter:   382 | Cost: 0.0136907\nIter:   383 | Cost: 0.0136607\nIter:   384 | Cost: 0.0136310\nIter:   385 | Cost: 0.0136015\nIter:   386 | Cost: 0.0135723\nIter:   387 | Cost: 0.0135433\nIter:   388 | Cost: 0.0135146\nIter:   389 | Cost: 0.0134863\nIter:   390 | Cost: 0.0134581\nIter:   391 | Cost: 0.0134303\nIter:   392 | Cost: 0.0134027\nIter:   393 | Cost: 0.0133755\nIter:   394 | Cost: 0.0133485\nIter:   395 | Cost: 0.0133218\nIter:   396 | Cost: 0.0132954\nIter:   397 | Cost: 0.0132694\nIter:   398 | Cost: 0.0132436\nIter:   399 | Cost: 0.0132181\nIter:   400 | Cost: 0.0131929\nIter:   401 | Cost: 0.0131681\nIter:   402 | Cost: 0.0131435\nIter:   403 | Cost: 0.0131193\nIter:   404 | Cost: 0.0130953\nIter:   405 | Cost: 0.0130717\nIter:   406 | Cost: 0.0130484\nIter:   407 | Cost: 0.0130254\nIter:   408 | Cost: 0.0130028\nIter:   409 | Cost: 0.0129804\nIter:   410 | Cost: 0.0129584\nIter:   411 | Cost: 0.0129367\nIter:   412 | Cost: 0.0129153\nIter:   413 | Cost: 0.0128942\nIter:   414 | Cost: 0.0128735\nIter:   415 | Cost: 0.0128530\nIter:   416 | Cost: 0.0128329\nIter:   417 | Cost: 0.0128131\nIter:   418 | Cost: 0.0127935\nIter:   419 | Cost: 0.0127743\nIter:   420 | Cost: 0.0127554\nIter:   421 | Cost: 0.0127368\nIter:   422 | Cost: 0.0127185\nIter:   423 | Cost: 0.0127006\nIter:   424 | Cost: 0.0126829\nIter:   425 | Cost: 0.0126655\nIter:   426 | Cost: 0.0126483\nIter:   427 | Cost: 0.0126315\nIter:   428 | Cost: 0.0126150\nIter:   429 | Cost: 0.0125987\nIter:   430 | Cost: 0.0125827\nIter:   431 | Cost: 0.0125670\nIter:   432 | Cost: 0.0125516\nIter:   433 | Cost: 0.0125364\nIter:   434 | Cost: 0.0125215\nIter:   435 | Cost: 0.0125068\nIter:   436 | Cost: 0.0124924\nIter:   437 | Cost: 0.0124782\nIter:   438 | Cost: 0.0124643\nIter:   439 | Cost: 0.0124507\nIter:   440 | Cost: 0.0124372\nIter:   441 | Cost: 0.0124240\nIter:   442 | Cost: 0.0124110\nIter:   443 | Cost: 0.0123983\nIter:   444 | Cost: 0.0123857\nIter:   445 | Cost: 0.0123734\nIter:   446 | Cost: 0.0123613\nIter:   447 | Cost: 0.0123494\nIter:   448 | Cost: 0.0123377\nIter:   449 | Cost: 0.0123262\nIter:   450 | Cost: 0.0123149\nIter:   451 | Cost: 0.0123038\nIter:   452 | Cost: 0.0122929\nIter:   453 | Cost: 0.0122821\nIter:   454 | Cost: 0.0122715\nIter:   455 | Cost: 0.0122611\nIter:   456 | Cost: 0.0122509\nIter:   457 | Cost: 0.0122409\nIter:   458 | Cost: 0.0122310\nIter:   459 | Cost: 0.0122212\nIter:   460 | Cost: 0.0122116\nIter:   461 | Cost: 0.0122022\nIter:   462 | Cost: 0.0121929\nIter:   463 | Cost: 0.0121838\nIter:   464 | Cost: 0.0121748\nIter:   465 | Cost: 0.0121660\nIter:   466 | Cost: 0.0121572\nIter:   467 | Cost: 0.0121487\nIter:   468 | Cost: 0.0121402\nIter:   469 | Cost: 0.0121319\nIter:   470 | Cost: 0.0121237\nIter:   471 | Cost: 0.0121156\nIter:   472 | Cost: 0.0121076\nIter:   473 | Cost: 0.0120998\nIter:   474 | Cost: 0.0120921\nIter:   475 | Cost: 0.0120844\nIter:   476 | Cost: 0.0120769\nIter:   477 | Cost: 0.0120695\nIter:   478 | Cost: 0.0120622\nIter:   479 | Cost: 0.0120550\nIter:   480 | Cost: 0.0120479\nIter:   481 | Cost: 0.0120409\nIter:   482 | Cost: 0.0120340\nIter:   483 | Cost: 0.0120272\nIter:   484 | Cost: 0.0120205\nIter:   485 | Cost: 0.0120138\nIter:   486 | Cost: 0.0120073\nIter:   487 | Cost: 0.0120008\nIter:   488 | Cost: 0.0119944\nIter:   489 | Cost: 0.0119881\nIter:   490 | Cost: 0.0119819\nIter:   491 | Cost: 0.0119758\nIter:   492 | Cost: 0.0119697\nIter:   493 | Cost: 0.0119637\nIter:   494 | Cost: 0.0119578\nIter:   495 | Cost: 0.0119520\nIter:   496 | Cost: 0.0119462\nIter:   497 | Cost: 0.0119405\nIter:   498 | Cost: 0.0119349\nIter:   499 | Cost: 0.0119293\n```\n:::\n\nFinally, we collect the predictions of the trained model for 50 values\nin the range $[-1,1]$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_pred = np.linspace(-1, 1, 50)\npredictions = [quantum_neural_net(var, x_) for x_ in x_pred]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and plot the shape of the function that the model has \"learned\" from the\nnoisy data (green dots).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.scatter(X, Y)\nplt.scatter(x_pred, predictions, color=\"green\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.tick_params(axis=\"both\", which=\"major\")\nplt.tick_params(axis=\"both\", which=\"minor\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](../demonstrations/quantum_neural_net/qnn_output_28_0.png)\n\nThe model has learned to smooth the noisy data.\n\nIn fact, we can use PennyLane to look at typical functions that the\nmodel produces without being trained at all. The shape of these\nfunctions varies significantly with the variance hyperparameter for the\nweight initialization.\n\nSetting this hyperparameter to a small value produces almost linear\nfunctions, since all quantum gates in the variational circuit\napproximately perform the identity transformation in that case. Larger\nvalues produce smoothly oscillating functions with a period that depends\non the number of layers used (generically, the more layers, the smaller\nthe period).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variance = 1.0\n\nplt.figure()\nx_pred = np.linspace(-2, 2, 50)\nfor i in range(7):\n    rnd_var = variance * np.random.randn(num_layers, 7)\n    predictions = [quantum_neural_net(rnd_var, x_) for x_ in x_pred]\n    plt.plot(x_pred, predictions, color=\"black\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.tick_params(axis=\"both\", which=\"major\")\nplt.tick_params(axis=\"both\", which=\"minor\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](../demonstrations/quantum_neural_net/qnn_output_30_0.png)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}